[CRAB]

jobtype = cmssw

scheduler  = glite

# use_server = 1

# server_name  = cern

[CMSSW]

### The data you want to access (to be found on DBS)
# nominal scale
# datasetpath=/DYJetsToLL_TuneZ2_M-50_7TeV-madgraph-tauola/Fall11-PU_S6-START44_V5-v1/AODSIM
datasetpath=/DYJetsToLL_TuneZ2_M-50_7TeV-madgraph-tauola/Fall11-PU_S6_START44_V9B-v1/AODSIM

# scale Dn
# datasetpath=/ZJetsToLL_TuneZ2_scaledown_7TeV-madgraph-tauola/Fall11-PU_S6_START44_V9B-v1/AODSIM

# scale Up
# datasetpath=/ZJetsToLL_TuneZ2_scaleup_7TeV-madgraph-tauola/Fall11-PU_S6_START44_V9B-v1/AODSIM

# VBF signal ~ 200K events
# datasetpath=/ZJetsToLL_TuneZ2_scaleup_7TeV-madgraph-tauola/Fall11-PU_S6_START44_V9B-v1/AODSIM

# EWK signal 1.5 M events
# datasetpath=/DYToLL_Mll-50_Mjj-120_7TeV_madgraph-pythia6/Fall11-PU_S6_START44_V9B-v1/AODSIM

### Total number of events to be accessed: -1 means all ("-1" is not usable if no input)

total_number_of_events=  1500000

events_per_job = 1500000

pset=DYjets_cfg.py

# The output files produced by your application (comma separated list)
output_file = dymadgraph.root

[USER]

# check_user_remote_dir = 0

#
#additional_input_files = /home_local/fanzago/fede.txt, ....
#
#ui_working_dir = /afs/cern.ch/user/a/anayak/scratch0/CMS/CRAB/test

#################################
#### JOB OUTPUT MANAGEMENT #####
##########################q#######

return_data = 0

copy_data = 1

storage_element=T2_UK_London_IC
user_remote_dir=DYjets_madgraph_scale0
#user_remote_dir=DYjets_madgraph_scaleDn
#user_remote_dir=DYjets_madgraph_scaleUp
#user_remote_dir=VBFH120
#user_remote_dir=EWKZ

#storage_element=srm-cms.cern.ch
#storage_path = /srm/managerv2?SFN=/castor/cern.ch
#user_remote_dir = /user/a/anikiten/DYjets_madgraph_scale0
#user_remote_dir = /user/a/anikiten/DYjets_madgraph_scaleDn
#user_remote_dir = /user/a/anikiten/DYjets_madgraph_scaleUp
#user_remote_dir = /user/a/anikiten/VBFH120
#user_remote_dir = /user/a/anikiten/EWKZ
email = Alexandre.Nikitenko@cern.ch


## the SE directory (or the mountpoint) that has to be writable from all
##(imp : Castor directory should be made public to store output root files,
##       using "rfchmod 775 dirName")
##  rfchmod 775 /castor/cern.ch/user/a/anikiten
##  rfchmod 775 /castor/cern.ch/user/a/anikiten/DiMuonData2011
##  rfchmod 775 /castor/cern.ch/user/a/anikiten/DYjets_sherpa
##  rfchmod 775 /castor/cern.ch/user/a/anikiten/DYjets_madgraph

[GRID]
## WMS to use

rb = CERN

proxy_server  = myproxy.cern.ch

virtual_organization    = cms

retry_count             = 2

lcg_catalog_type        = lfc

lfc_host                = lfc-cms-test.cern.ch

lfc_home                = /grid/cms

##  Black and White Lists management:
## by SE 
#se_black_list =
#se_white_list =
## by CE
#ce_black_list =
#ce_white_list =

## Role in VOMS
#role = superman
## Group in VOMS
#group = superheros

## cpu time and wall_clock_time(=real time) in minutes. Written into the jdl file
#max_cpu_time = 60
#max_wall_clock_time = 60


