[CRAB]

jobtype = cmssw

scheduler  = glite

use_server = 1

# server_name  = cern

[CMSSW]

### The data you want to access (to be found on DBS)

# change dataset
# background
# DY
# datasetpath=/DYJetsToLL_TuneZ2_M-50_7TeV-madgraph-tauola/Fall11-PU_S6-START44_V5-v1/AODSIM
# datasetpath=/DYJetsToLL_TuneZ2_M-50_7TeV-madgraph-tauola/Fall11-PU_S6_START44_V9B-v1/AODSIM
# tt
# datasetpath=/TTJets_TuneZ2_7TeV-madgraph-tauola/Fall11-PU_S6_START44_V9B-v1/AODSIM
# WW
# datasetpath=/WW_TuneZ2_7TeV_pythia6_tauola/Fall11-PU_S6_START44_V9B-v1/AODSIM
# WZ
# datasetpath=/WZ_TuneZ2_7TeV_pythia6_tauola/Fall11-PU_S6_START44_V9B-v1/AODSIM
# ZZ
# datasetpath=/ZZ_TuneZ2_7TeV_pythia6_tauola/Fall11-PU_S6_START44_V9B-v1/AODSIM
# signal: 
# datasetpath=/DYToLL_Mll-50_Mjj-120_7TeV_madgraph-pythia6/Fall11-PU_S6_START44_V9B-v1/AODSIM
#
datasetpath=/TTJets_MassiveBinDECAY_TuneZ2star_8TeV-madgraph-tauola/Summer12_DR53X-PU_S10_START53_V7A-v1/AODSIM

### Total number of events to be accessed: -1 means all ("-1" is not usable if no input)

# change n events
# DY
# total_number_of_events=  15000000
# tt
# total_number_of_events=  3000000
# WW
# total_number_of_events=  3000000
# WZ
# total_number_of_events=  3000000
# ZZ
# total_number_of_events=  3000000
# signal
# total_number_of_events=  1500000
# tt~ 
total_number_of_events=  60000
#
# change events per job
# DY
# events_per_job = 35000
# tt
# events_per_job = 15000
# WW
# events_per_job = 15000
# WZ
# events_per_job = 15000
# ZZ
# events_per_job = 15000
# signal
events_per_job = 10000

pset=DiMuAnalysis_MC_cfg.py

# The output files produced by your application (comma separated list)
output_file = VBFHinvis.root

[USER]

# check_user_remote_dir = 0

#
#additional_input_files = /home_local/fanzago/fede.txt, ....
#
#ui_working_dir = /afs/cern.ch/user/a/anayak/scratch0/CMS/CRAB/test

#################################
#### JOB OUTPUT MANAGEMENT #####
##########################q#######

return_data = 0

copy_data = 1

# IC
# change SE

storage_element=T2_UK_London_IC
# user_remote_dir=DiMuonMCFall11
# user_remote_dir=TTFall11
# user_remote_dir=WWFall11
# user_remote_dir=WZFall11
# user_remote_dir=ZZFall11
# user_remote_dir=SignalMCFall11
user_remote_dir=TTCMSSW530

#storage_element = gfe02.grid.hep.ph.ic.ac.uk
#storage_path =  /srm/managerv1?SFN=/pnfs/hep.ph.ic.ac.uk/data/cms/store
#user_remote_dir = /user/anikiten/DiMuonMCFall11

# storage_element=srm-cms.cern.ch
# storage_path = /srm/managerv2?SFN=/castor/cern.ch
# user_remote_dir = /user/a/anikiten/DiMuonMCFall11
# user_remote_dir = /user/a/anikiten/SignalMCFall11
email = Alexandre.Nikitenko@cern.ch


## the SE directory (or the mountpoint) that has to be writable from all
##(imp : Castor directory should be made public to store output root files,
##       using "rfchmod 775 dirName")
##  rfchmod 775 /castor/cern.ch/user/a/anikiten
##  rfchmod 775 /castor/cern.ch/user/a/anikiten/DiMuonMCFall11
##  rfchmod 775 /castor/cern.ch/user/a/anikiten/SignalMCFall11

[GRID]
## WMS to use

rb = CERN

proxy_server  = myproxy.cern.ch

virtual_organization    = cms

retry_count             = 2

lcg_catalog_type        = lfc

lfc_host                = lfc-cms-test.cern.ch

lfc_home                = /grid/cms

##  Black and White Lists management:
## by SE 
se_black_list = T2_RU_JINR
# se_white_list = T2_DE_RWTH
## by CE
ce_black_list = T2_RU_JINR
# ce_white_list = T2_DE_RWTH

## Role in VOMS
#role = superman
## Group in VOMS
#group = superheros

## cpu time and wall_clock_time(=real time) in minutes. Written into the jdl file
#max_cpu_time = 60
#max_wall_clock_time = 60


